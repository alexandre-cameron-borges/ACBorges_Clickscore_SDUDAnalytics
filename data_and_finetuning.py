# -*- coding: utf-8 -*-
"""data_and_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrgvBJ_1BHrT732r1RnnV1ZBOvZKzZN5
"""

# Commented out IPython magic to ensure Python compatibility.
# 1) Cloner le repo & installer les outils
!git clone https://github.com/alexandre-cameron-borges/duda_clickscore.git
# %cd data_and_finetuning
!pip install -q -r requirements.txt nbstripout kaggle

# 2) Nettoyer les notebooks avant commit
!nbstripout --install

# 3) Monter Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 0) Debug
!pip install -q 'jupyter-client<8.0.0'
!pip install -q 'torch==2.6.0'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

## A FAIRE QUE LA PREMIERE FOIS

# 4) Configurer l’API Kaggle
!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/credentials/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# 5) Créer le dossier raw
!mkdir -p /content/drive/MyDrive/data/raw

# 6) Télécharger & dézipper les jeux Kaggle
!kaggle datasets download -d amananandrai/clickbait-dataset \
    -p /content/drive/MyDrive/data/raw --unzip
!kaggle datasets download -d marius2303/ad-click-prediction-dataset \
    -p /content/drive/MyDrive/data/raw --unzip
!kaggle datasets download -d souvik1618/ad-click-prediction-dataset \
    -p /content/drive/MyDrive/data/raw --unzip

# 7) Télécharger + unzip Webis Clickbait Corpus 2017
!wget -q -O /content/drive/MyDrive/data/raw/clickbait17-train-170630.zip "https://zenodo.org/record/5530410/files/clickbait17-train-170630.zip?download=1"
!unzip -q /content/drive/MyDrive/data/raw/clickbait17-train-170630.zip -d /content/drive/MyDrive/data/raw/webis_clickbait
!rm /content/drive/MyDrive/data/raw/clickbait17-train-170630.zip

# Chemin vers le CSV amandarai clickbaitdataset dans ton Drive
pathtableclickbait_a = "/content/drive/MyDrive/data/raw/clickbait_data.csv"
tableclickbait_a = pd.read_csv(pathtableclickbait_a)

# Changement de la colonne de clickbait en Clickbait et des valeurs 0 en no-clickbait, 1 en clickbait et de headline en Texte
tableclickbait_a['clickbait'] = tableclickbait_a['clickbait'].map({1: 'clickbait', 0: 'no-clickbait'})
tableclickbait_a.rename(columns={'clickbait': 'Clickbait'}, inplace=True)
tableclickbait_a.rename(columns={'headline': 'Texte'}, inplace=True)


# Aperçu rapide
print("Shape :", tableclickbait_a.shape)
display(tableclickbait_a.head(8))

# Comptage des nulls et types
display(tableclickbait_a.info())
display(tableclickbait_a.isnull().sum())

# Répartition clickbait vs non-clickbait
counts = tableclickbait_a['Clickbait'].value_counts()
print(counts)

# Chemin vers le CSV marius clickbaitdataset dans ton Drive
pathtableclickbait_b = "/content/drive/MyDrive/data/raw/ad_click_dataset.csv"
tableclickbait_b = pd.read_csv(pathtableclickbait_b)

# Nettoyage des NaN et des Non-Binaire
tableclickbait_b = tableclickbait_b[tableclickbait_b['gender'].isin(['Male', 'Female'])]
tableclickbait_b = tableclickbait_b.dropna(subset=['age'])

# Changement colonne 'click' en 'clickbait' et les valeurs 0 en no-clickbait, 1 en clickbait
tableclickbait_b['click'] = tableclickbait_b['click'].map({1: 'clickbait', 0: 'no-clickbait'})
tableclickbait_b.rename(columns={'click': 'Clickbait'}, inplace=True)

# Aperçu rapide
print("Shape :", tableclickbait_b.shape)
display(tableclickbait_b.head(8))

# Comptage des nulls et types
display(tableclickbait_b.info())
display(tableclickbait_b.isnull().sum())

# Répartition clickbait vs non-clickbait
counts = tableclickbait_b['Clickbait'].value_counts()
print(counts)

# Chemin vers le CSV souvik clickbaitdataset dans ton Drive
pathtableclickbait_c = "/content/drive/MyDrive/data/raw/advertising.csv"
tableclickbait_c = pd.read_csv(pathtableclickbait_c)

# Changement colonne Male en Gender et les valeurs 0 en Female, 1 en Male et Ad Topic Line en Texte et Age en age
tableclickbait_c['Male'] = tableclickbait_c['Male'].map({1: 'Male', 0: 'Female'})
tableclickbait_c.rename(columns={'Male': 'gender'}, inplace=True)
tableclickbait_c.rename(columns={'Age': 'age'}, inplace=True)
tableclickbait_c.rename(columns={'Ad Topic Line': 'Texte'}, inplace=True)


# Changement colonne 'clicked on ad' en 'clickbait' et les valeurs 0 en no-clickbait, 1 en clickbait
tableclickbait_c['Clicked on Ad'] = tableclickbait_c['Clicked on Ad'].map({1: 'clickbait', 0: 'no-clickbait'})
tableclickbait_c.rename(columns={'Clicked on Ad': 'Clickbait'}, inplace=True)

# Aperçu rapide
print("Shape :", tableclickbait_c.shape)
display(tableclickbait_c.head(8))

# Comptage des nulls et types
display(tableclickbait_c.info())
display(tableclickbait_c.isnull().sum())

# Répartition clickbait vs non-clickbait
counts = tableclickbait_c['Clickbait'].value_counts()
print(counts)

# 1) Chemin vers fichier webis JSONL & 2) Charger le JSONL
pathtableclickbait_d = "/content/drive/MyDrive/data/raw/webis_clickbait/clickbait17-large-validation-170630/truth.jsonl"
pathtableclickbait_d2 = "/content/drive/MyDrive/data/raw/webis_clickbait/clickbait17-large-validation-170630/instances.jsonl"
tableclickbait_d = pd.read_json(pathtableclickbait_d, lines=True)
tableclickbait_d2 = pd.read_json(pathtableclickbait_d2, lines=True)

# Changement colonne 'truthClass' en 'clickbait' et targetTitle en Texte
tableclickbait_d.rename(columns={'truthClass': 'Clickbait'}, inplace=True)
tableclickbait_d2.rename(columns={'targetTitle': 'Texte'}, inplace=True)


# Aperçu rapide
print("Shape :", tableclickbait_d.shape)
display(tableclickbait_d.head(8))

print("Shape :", tableclickbait_d2.shape)
display(tableclickbait_d2.head(8))

# Comptage des nulls et types
display(tableclickbait_d.info())
display(tableclickbait_d.isnull().sum())

display(tableclickbait_d2.info())
display(tableclickbait_d2.isnull().sum())

# Répartition clickbait vs non-clickbait
counts = tableclickbait_d['Clickbait'].value_counts()
print(counts)

#Filtre des colonnes utiles

cols_a   = ['Texte', 'Clickbait']
cols_b   = ['age', 'gender', 'Clickbait']
cols_c   = ['age', 'gender', 'Texte', 'Clickbait']
cols_d   = ['Clickbait','truthMean', 'id']
cols_d2  = ['Texte', 'postMedia', 'id']

tableclickbait_a  = tableclickbait_a[cols_a]
tableclickbait_b  = tableclickbait_b[cols_b]
tableclickbait_c  = tableclickbait_c[cols_c]
tableclickbait_d  = tableclickbait_d[cols_d]
tableclickbait_d2 = tableclickbait_d2[cols_d2]

display(tableclickbait_a.head(5))
display(tableclickbait_b.head(5))
display(tableclickbait_c.head(5))
display(tableclickbait_d.head(5))
display(tableclickbait_d2.head(5))

#Fusion de D & D2 (Webis)

tableclickbait_dd2 = pd.merge(
    tableclickbait_d,
    tableclickbait_d2,
    on="id",
    how="inner"
)

#Filtre
cols_dd2  = ['Texte','truthMean','Clickbait']
tableclickbait_dd2 = tableclickbait_dd2[cols_dd2]


display(tableclickbait_dd2.head(30))
display(tableclickbait_dd2.info())

#Fusion de B & C

tableclickbait_bc = tableclickbait_b.merge(
    tableclickbait_c,
    on=["age", "gender", "Clickbait"],
    how="outer",
    suffixes=("_b", "_c")
)

display(tableclickbait_bc.head(5))
display(tableclickbait_bc.info())

# Fusion des tables A & D D2

tableclickbait_add2 = tableclickbait_a.merge(
    tableclickbait_dd2,
    on=["Texte", "Clickbait"],
    how="outer",
    suffixes=("_a", "_dd2")
)

# 1. Créer les masques
mask_na     = tableclickbait_add2['truthMean'].isna()
mask_non_cb = mask_na & (tableclickbait_add2['Clickbait'] == 'non-clickbait')
mask_cb     = mask_na & (tableclickbait_add2['Clickbait'] != 'non-clickbait')

# 2. Pour reproductibilité (facultatif)
np.random.seed(42)

# 3. Assignation aléatoire
tableclickbait_add2.loc[mask_non_cb, 'truthMean'] = \
    np.random.uniform(0.0, 0.5, size=mask_non_cb.sum())

tableclickbait_add2.loc[mask_cb, 'truthMean'] = \
    np.random.uniform(0.5, 1.0, size=mask_cb.sum())

# 4. Vérification rapide
print("Non-clickbait remplies :", mask_non_cb.sum())
print("Clickbait/other remplies :", mask_cb.sum())
display(tableclickbait_add2.head(30))
display(tableclickbait_add2.info())

# Fusion de toutes les tables A B C D D2

tableclickbait_abcdd2 = tableclickbait_add2.merge(
    tableclickbait_bc,
    on=["Texte", "Clickbait"],
    how="outer",
    suffixes=("_add2", "_bc")
)


# 1. Masques généraux
mask_na       = tableclickbait_abcdd2['truthMean'].isna()
mask_age_na   = tableclickbait_abcdd2['age'].isna()
mask_gen_na   = tableclickbait_abcdd2['gender'].isna()

# 2. Sous-masques selon Clickbait
is_non_cb = tableclickbait_abcdd2['Clickbait'] == 'non-clickbait'
mask_tm_nc = mask_na   & is_non_cb
mask_tm_cb = mask_na   & ~is_non_cb
mask_age_nc = mask_age_na & is_non_cb
mask_age_cb = mask_age_na & ~is_non_cb
mask_gen_nc = mask_gen_na & is_non_cb
mask_gen_cb = mask_gen_na & ~is_non_cb

# 3. Seed pour reproductibilité (facultatif)
np.random.seed(42)

# 4. Assignations
# — truthMean
tableclickbait_abcdd2.loc[mask_tm_nc, 'truthMean'] = \
    np.random.uniform(0.0, 0.5, size=mask_tm_nc.sum())
tableclickbait_abcdd2.loc[mask_tm_cb, 'truthMean'] = \
    np.random.uniform(0.5, 1.0, size=mask_tm_cb.sum())

# — age (entiers) : non-clickbait [18,40), clickbait [40,80)
tableclickbait_abcdd2.loc[mask_age_nc, 'age'] = \
    np.random.randint(18, 40, size=mask_age_nc.sum())
tableclickbait_abcdd2.loc[mask_age_cb, 'age'] = \
    np.random.randint(40, 80, size=mask_age_cb.sum())

# — gender (catégories) : tirage aléatoire
choices = ['male', 'female']
# vous pouvez ajuster p=[...] selon vos hypothèses
tableclickbait_abcdd2.loc[mask_gen_nc, 'gender'] = \
    np.random.choice(choices, size=mask_gen_nc.sum(), p=[0.5, 0.5])
tableclickbait_abcdd2.loc[mask_gen_cb, 'gender'] = \
    np.random.choice(choices, size=mask_gen_cb.sum(), p=[0.5, 0.5])

# 5. Vérification rapide
print(f"truthMean remplies : {mask_tm_nc.sum()+mask_tm_cb.sum()}")
print(f"age remplies       : {mask_age_nc.sum()+mask_age_cb.sum()}")
print(f"gender remplies    : {mask_gen_nc.sum()+mask_gen_cb.sum()}")
display(tableclickbait_abcdd2.head(10))
display(tableclickbait_abcdd2.info())

# 1) Chemin vers fichier mind
pathtableclickbait_e = "/content/drive/MyDrive/data/raw/mind/MINDlarge_train/behaviors.tsv"
pathtableclickbait_e2 = "/content/drive/MyDrive/data/raw/mind/MINDlarge_train/news.tsv"

tableclickbait_e = pd.read_csv(pathtableclickbait_e, sep='\t', header=None) # header=None to avoid first row as header
tableclickbait_e2 = pd.read_csv(pathtableclickbait_e2, sep='\t', header=None) # header=None to avoid first row as header

# Récupérer les colonnes d’indice 0 et 3 pour chacun
tableclickbait_e   = tableclickbait_e.iloc[:, 4]
tableclickbait_e2  = tableclickbait_e2.iloc[:, [0, 3]]

# Aperçu rapide
print("Shape :", tableclickbait_e.shape)
display(tableclickbait_e.head(8))

print("Shape :", tableclickbait_e2.shape)
display(tableclickbait_e2.head(8))

# Comptage des nulls et types
display(tableclickbait_e.info())
display(tableclickbait_e.isnull().sum())

display(tableclickbait_e2.info())
display(tableclickbait_e2.isnull().sum())

# Répartition clickbait vs non-clickbait
#counts = tableclickbait_e['Clickbait'].value_counts() # This line will likely cause an error
#print(counts)

# --- 1) Raw impressions Series
raw = tableclickbait_e.astype(str)

# --- 2) Décomposer en (ad_id, clicked)
tokens = (
    raw.str
       .split()                      # sépare sur espace
       .explode()                    # 1 token par ligne
       .str.split('-', expand=True)  # sépare ID et click
       .rename(columns={0:"ad_id", 1:"clicked"})
)
tokens["clicked"] = tokens["clicked"].astype(int)

# --- 3) Aggréger pour calculer impressions & CTR
ctr_df = (
    tokens.groupby("ad_id")["clicked"]
          .agg(impressions="size", ctr="mean")
          .reset_index()
)

# --- 4) Fusionner avec la table des pubs (ID en colonne 0 de tableclickbait_e2)
result = (
    tableclickbait_e2
      .merge(ctr_df, left_on=tableclickbait_e2.columns[0], right_on="ad_id", how="left")
      .drop(columns="ad_id")
)

# 4,5) Remplacer les NaN
result["impressions"] = result["impressions"].fillna(0).astype(int)
result["ctr"]         = result["ctr"].fillna(0.0)

# --- 5) (Optionnel) Renommer pour plus de lisibilité
result.columns = ["news_id", "title", "impressions", "ctr"]

# --- 2) Filtrer pour ne garder que les pubs vues au moins une fois ---
result_seen = result[result["impressions"] > 0].reset_index(drop=True)

# 2) Tri par CTR décroissant
result_seen_sorted = result_seen.sort_values("ctr", ascending=False).reset_index(drop=True)


# À partir de result_seen_sorted trié par CTR
result_top100 = result_seen_sorted[result_seen_sorted["impressions"] >= 100] \
                   .reset_index(drop=True)

result_top100 = result_top100[['title', 'ctr']]

# Aperçu
print((result_top100).head(20))
display(result_top100.info())
display(result_top100.isnull().sum())

counts_ctr = result_top100['ctr'].value_counts()
print(counts_ctr)

q1, q2, q3 = result_top100['ctr'].quantile([0.25, 0.50, 0.75])
print(f"1er quartile (25 %) : {q1:.3f}")
print(f"2ᵉ quartile (médiane) : {q2:.3f}")
print(f"3ᵉ quartile (75 %) : {q3:.3f}")


plt.figure()
plt.plot(result_top100['ctr'])
plt.xlabel('Index des pubs')
plt.ylabel('CTR')
plt.title('Distribution linéaire du CTR pour chaque pub (≥ 100 impressions)')
plt.tight_layout()
plt.show()

# Cellule 11: Nettoyage & features simples MODELE CLICKBAIT
df = tableclickbait_abcdd2.dropna(subset=['Texte']).copy()

# 1. Label binaire
df['label'] = df['Clickbait'].map({'clickbait':1,'no-clickbait':0})

# 2. Imputation & normalisation de l'âge
med_age        = df['age'].median()
df['age']      = df['age'].fillna(med_age)
df['age_norm'] = (df['age'] - med_age) / (df['age'].max() - med_age)

# 3. Imputation & normalisation de truthMean
med_tm            = df['truthMean'].median()
df['truthMean']   = df['truthMean'].fillna(med_tm)
df['truthMean_norm'] = (df['truthMean'] - med_tm) / (df['truthMean'].max() - med_tm)

# 3b. Binning de truthMean en 3 classes
bins = [-np.inf, 0.6, 0.8, np.inf]
labels = ['low','mid','high']
df['tm_cat'] = pd.cut(df['truthMean'], bins=bins, labels=labels)
df['tm_id']  = df['tm_cat'].cat.codes  # 0,1,2

# 4. Genre manquant → 'Unknown', puis encodage
df['gender']    = df['gender'].fillna('Unknown')
df['gender_id'] = df['gender'].astype('category').cat.codes

# 4,5. Encoder en entier (0=low, 1=mid, 2=high)
df['tm_id'] = df['tm_cat'].cat.codes

# 5. Vérification rapide
print("Après nettoyage :", df.shape)
display(df[['Texte','label','age_norm','truthMean_norm','gender_id']].head())
display(df.value_counts('label'))

# Cellule 12: train/test split
from sklearn.model_selection import train_test_split

# 1. Clé combinée pour assurer l’équilibre sur les deux cibles
df['stratify_key'] = df['label'].astype(str) + '_' + df['tm_id'].astype(str)

# 2. Split train/val stratifié sur la clé combinée
train_df, val_df = train_test_split(
    df,
    stratify=df['stratify_key'],
    test_size=0.2,
    random_state=42
)

print("Train:", train_df.shape, "– Val:", val_df.shape)

# (Optionnel) Supprimer la colonne temporaire
train_df = train_df.drop(columns='stratify_key')
val_df   = val_df.drop(columns='stratify_key')

# 0. Pré-requis : seuils et labels pour tm_id
bins      = [-np.inf, 0.6, 0.8, np.inf]
labels_tm = ['low', 'mid', 'high']

# 1. Calculez med_tm si pas déjà fait
med_tm = df['truthMean'].median()

# 2. Créez votre DataFrame de bruit
shorts = ["..", ",,", "zzz", "?!", "  ", "test"] * 1000
df_bruit = pd.DataFrame({
    "Texte":          shorts,
    "label":          [0]*len(shorts),
    "age_norm":       0.0,
    "gender_id":      2,
    "truthMean":      0,
    "truthMean_norm": 0.0,
})

# 3. Binning et encodage de truthMean → tm_id
df_bruit['tm_cat'] = pd.cut(df_bruit['truthMean'], bins=bins, labels=labels_tm)
df_bruit['tm_id']  = df_bruit['tm_cat'].cat.codes

# 4. Création de la clé de stratification
df_bruit['stratify_key'] = df_bruit['label'].astype(str) + '_' + df_bruit['tm_id'].astype(str)

# 5. Concaténation finale à train_df
train_df = pd.concat([train_df, df_bruit], axis=0).reset_index(drop=True)

# Cellule 13: Dataset & DataLoader
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

class CB_Dataset(Dataset):
    def __init__(self, df):
        self.texts    = df["Texte"].tolist()
        self.ages     = df["age_norm"].tolist()
        self.genders  = df["gender_id"].tolist()
        self.labels   = df["label"].tolist()
        self.tm_ids   = df["tm_id"].tolist()           # ← ajout

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, i):
        enc = tokenizer(
            self.texts[i],
            padding="max_length",
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        return {
            "input_ids":      enc.input_ids.squeeze(0),
            "attention_mask": enc.attention_mask.squeeze(0),
            "age":            torch.tensor(self.ages[i],   dtype=torch.float),
            "gender":         torch.tensor(self.genders[i],dtype=torch.long),
            "label":          torch.tensor(self.labels[i], dtype=torch.float),
            "tm_id":          torch.tensor(self.tm_ids[i], dtype=torch.long)  # ← ajout
        }

# Les DataLoader restent identiques
train_loader = DataLoader(CB_Dataset(train_df), batch_size=16, shuffle=True)
val_loader   = DataLoader(CB_Dataset(val_df),   batch_size=16)
print("Batches train:", len(train_loader), "– val :", len(val_loader))

# Cellule 14 – Modèle multi‐tâche Clickbait + tm_id
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, get_linear_schedule_with_warmup
from sklearn.metrics import f1_score, accuracy_score
from tqdm.auto import tqdm

# 1) Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2) Architecture multi‐tâche
class ClickbaitModel(nn.Module):
    def __init__(self, n_genders, n_tm_classes=3):
        super().__init__()
        self.bert       = BertModel.from_pretrained("bert-base-multilingual-cased")
        self.age_fc     = nn.Linear(1, 16)
        self.gender_emb = nn.Embedding(n_genders, 8)
        hid = self.bert.config.hidden_size + 16 + 8

        # tête binaire clickbait
        self.cb_head = nn.Sequential(
            nn.Linear(hid, 64), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(64, 1)
        )
        # tête multi‐classe tm_id
        self.tm_head = nn.Sequential(
            nn.Linear(hid, 64), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(64, n_tm_classes)
        )

    def forward(self, input_ids, attention_mask, age, gender):
        pooled = self.bert(input_ids, attention_mask).pooler_output
        a      = F.relu(self.age_fc(age.unsqueeze(1)))
        g      = self.gender_emb(gender)
        x      = torch.cat([pooled, a, g], dim=1)
        return self.cb_head(x).squeeze(-1), self.tm_head(x)

model = ClickbaitModel(n_genders=train_df['gender_id'].nunique()).to(device)

# 3) Pertes & optim
num_epochs  = 10
total_steps = len(train_loader) * num_epochs
neg_ratio   = 1 - train_df['label'].mean()
pos_ratio   = train_df['label'].mean()
pos_weight  = torch.tensor([neg_ratio/pos_ratio], device=device)

cb_crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
tm_crit = nn.CrossEntropyLoss()
λ       = 0.5

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
scheduler = get_linear_schedule_with_warmup(optimizer,
    num_warmup_steps=int(0.2*total_steps),
    num_training_steps=total_steps
)

# 4) Entraînement + évaluation
best_f1, patience = 0, 0
for epoch in range(num_epochs):
    model.train()
    for batch in tqdm(train_loader, desc=f"Train Ep{epoch+1}"):
        batch = {k:v.to(device) for k,v in batch.items()}
        optimizer.zero_grad()
        logits_cb, logits_tm = model(
            batch['input_ids'], batch['attention_mask'],
            batch['age'], batch['gender']
        )
        loss = λ*cb_crit(logits_cb, batch['label']) + (1-λ)*tm_crit(logits_tm, batch['tm_id'])
        loss.backward(); optimizer.step(); scheduler.step()

    model.eval()
    preds, truths, tm_preds, tm_truths = [], [], [], []
    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f" Val Ep{epoch+1}"):
            batch = {k:v.to(device) for k,v in batch.items()}
            logits_cb, logits_tm = model(
                batch['input_ids'], batch['attention_mask'],
                batch['age'], batch['gender']
            )
            preds      += torch.sigmoid(logits_cb).round().cpu().tolist()
            truths     += batch['label'].cpu().tolist()
            tm_preds   += torch.argmax(logits_tm, dim=1).cpu().tolist()
            tm_truths  += batch['tm_id'].cpu().tolist()

    f1  = f1_score(truths, preds)
    acc = accuracy_score(tm_truths, tm_preds)
    print(f"Epoch {epoch+1} — Clickbait F1: {f1:.3f}, tm_id Acc: {acc:.3f}")

    if f1 > best_f1:
        best_f1, patience = f1, 0
        torch.save(model.state_dict(), "best_cb_model.pt")
    else:
        patience += 1
        if patience >= 3:
            print("Early stopping.")
            break

print("Best Clickbait F1:", best_f1)

# 1) Récupérer toutes les prédictions & probas pour les deux tâches
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_curve, auc, precision_recall_curve,
    accuracy_score
)
import matplotlib.pyplot as plt
import torch

model.eval()
probs_cb, truths_cb = [], []
logits_tm_list, truths_tm = [], []

with torch.no_grad():
    for batch in val_loader:
        batch = {k:v.to(device) for k,v in batch.items()}
        logits_cb, logits_tm = model(
            batch['input_ids'], batch['attention_mask'],
            batch['age'], batch['gender']
        )
        probs_cb     += torch.sigmoid(logits_cb).cpu().tolist()
        truths_cb    += batch['label'].cpu().tolist()
        logits_tm_list.append(logits_tm.cpu())
        truths_tm    += batch['tm_id'].cpu().tolist()

# Binarisation clickbait
preds_cb = [1 if p>=0.7 else 0 for p in probs_cb]

# Multi-classe tm_id
all_logits_tm = torch.cat(logits_tm_list, dim=0)
preds_tm      = torch.argmax(all_logits_tm, dim=1).tolist()

# 2a) Rapport & matrice pour clickbait
print("=== Clickbait ===")
print(classification_report(truths_cb, preds_cb, digits=4))
print("Confusion matrix:\n", confusion_matrix(truths_cb, preds_cb))

# 2b) Rapport & matrice pour tm_id
print("\n=== tm_id (3 classes) ===")
print(classification_report(truths_tm, preds_tm, digits=4))
print("Confusion matrix:\n", confusion_matrix(truths_tm, preds_tm))

# 3) ROC & PR pour clickbait
fpr, tpr, _ = roc_curve(truths_cb, probs_cb)
roc_auc     = auc(fpr, tpr)
plt.figure(); plt.plot(fpr, tpr, label=f"AUC={roc_auc:.3f}")
plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("ROC Curve"); plt.legend(); plt.show()

prec, rec, _ = precision_recall_curve(truths_cb, probs_cb)
pr_auc       = auc(rec, prec)
plt.figure(); plt.plot(rec, prec, label=f"PR-AUC={pr_auc:.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("Precision-Recall"); plt.legend(); plt.show()

# 4) Accuracy pour tm_id
acc_tm = accuracy_score(truths_tm, preds_tm)
print(f"tm_id accuracy: {acc_tm:.3f}")

!pip install huggingface_hub
!huggingface-cli login

import os
from huggingface_hub import HfApi
from google.colab import userdata


token = userdata.get('HUGGINGFACE_TOKEN')  # lit automatiquement ta variable d'env
api   = HfApi(token=token)               # tu passes maintenant le token ici

# 1) Créer le repo (pas besoin de repasser token)
api.create_repo(
    repo_id="alexandre-cameron-borges/clickbait-model",
    private=True,
    exist_ok=True
)

# 2) Uploader le .pt
api.upload_file(
    path_or_fileobj="best_cb_model.pt",
    path_in_repo="best_cb_model.pt",
    repo_id="alexandre-cameron-borges/clickbait-model"
)

#CTR modèle

from sklearn.model_selection import train_test_split

# result_top100 contient [‘title’, ‘ctr’]
train_ctr_df, val_ctr_df = train_test_split(
    result_top100, test_size=0.2, random_state=42
)
print("CTR train:", train_ctr_df.shape, "– CTR val:", val_ctr_df.shape)

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

class CTR_Dataset(Dataset):
    def __init__(self, df):
        self.texts = df["title"].tolist()
        self.targets = df["ctr"].tolist()
    def __len__(self):
        return len(self.targets)
    def __getitem__(self, i):
        enc = tokenizer(
            self.texts[i], padding="max_length", truncation=True,
            max_length=128, return_tensors="pt"
        )
        return {
            "input_ids": enc.input_ids.squeeze(0),
            "attention_mask": enc.attention_mask.squeeze(0),
            "ctr": torch.tensor(self.targets[i], dtype=torch.float)
        }

train_ctr_loader = DataLoader(CTR_Dataset(train_ctr_df), batch_size=16, shuffle=True)
val_ctr_loader   = DataLoader(CTR_Dataset(val_ctr_df),   batch_size=16)

import torch.nn as nn, torch.nn.functional as F

class CTRModel(nn.Module):
    def __init__(self):
        super().__init__()
        from transformers import BertModel
        self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
        h = self.bert.config.hidden_size
        self.head = nn.Sequential(
            nn.Linear(h, 64), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids, attention_mask).pooler_output
        logits = self.head(out).squeeze(-1)
        probabilities = torch.sigmoid(logits) # <--- VOICI L'AJOUT
        return probabilities

device_ctr     = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ctr_model  = CTRModel().to(device_ctr)

# Cellule X – Modèle CTR + entraînement amélioré
import torch
import torch.nn as nn
from tqdm.auto import tqdm
from transformers import BertModel, get_linear_schedule_with_warmup
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import mean_squared_error, mean_absolute_error

# --- 1) Architecture avec dropout renforcé
class CTRModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
        hid = self.bert.config.hidden_size
        self.head = nn.Sequential(
            nn.Linear(hid, 64),
            nn.ReLU(),
            nn.Dropout(0.2),    # dropout ↑
            nn.Linear(64, 1)
        )
    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids, attention_mask).pooler_output
        return self.head(out).squeeze(-1)

ctr_model = CTRModel().to(device_ctr)

# --- 2) Hyper-paramètres & scheduler
num_epochs    = 10
patience_max  = 3
learning_rate = 2e-5
total_steps   = len(train_ctr_loader) * num_epochs

optimizer_ctr = torch.optim.AdamW(
    ctr_model.parameters(),
    lr=learning_rate,
    weight_decay=0.01    # weight decay ajouté
)
scheduler_ctr = get_linear_schedule_with_warmup(
    optimizer_ctr,
    num_warmup_steps=int(0.2 * total_steps),  # warm-up 20%
    num_training_steps=total_steps
)

criterion = nn.SmoothL1Loss()  # huber-like

# --- 3) TensorBoard
writer = SummaryWriter(log_dir="runs/ctr_experiment")

best_rmse = float('inf')
patience  = 0
step      = 0

for epoch in range(1, num_epochs+1):
    # Entraînement
    ctr_model.train()
    bar = tqdm(train_ctr_loader, desc=f"Train Ep{epoch}", leave=False)
    for batch in bar:
        optimizer_ctr.zero_grad()
        inputs = batch["input_ids"].to(device_ctr)
        masks  = batch["attention_mask"].to(device_ctr)
        targets= batch["ctr"].to(device_ctr)

        preds = ctr_model(inputs, masks)
        loss  = criterion(preds, targets)
        loss.backward()
        optimizer_ctr.step()
        scheduler_ctr.step()

        writer.add_scalar("train/loss", loss.item(), step)
        step += 1
        bar.set_postfix(loss=f"{loss.item():.4f}")

    # Validation
    ctr_model.eval()
    all_preds, all_trues = [], []
    with torch.no_grad():
        for batch in val_ctr_loader:
            inputs = batch["input_ids"].to(device_ctr)
            masks  = batch["attention_mask"].to(device_ctr)
            targets= batch["ctr"].to(device_ctr)
            preds = ctr_model(inputs, masks)
            all_preds.extend(preds.cpu().tolist())
            all_trues.extend(targets.cpu().tolist())

    mse  = mean_squared_error(all_trues, all_preds)
    rmse = mse ** 0.5
    mae  = mean_absolute_error(all_trues, all_preds)

    writer.add_scalar("val/rmse", rmse, epoch)
    writer.add_scalar("val/mae", mae, epoch)
    print(f"Epoch {epoch}: Val RMSE={rmse:.4f}, MAE={mae:.4f}")

    # Early stopping & checkpoint
    if rmse < best_rmse:
        best_rmse = rmse
        patience  = 0
        torch.save(ctr_model.state_dict(), "best_ctr_model.pt")
    else:
        patience += 1
        if patience >= patience_max:
            print("Early stopping triggered.")
            break

writer.close()

import os
from huggingface_hub import HfApi
from google.colab import userdata


token = userdata.get('HUGGINGFACE_TOKEN')  # lit automatiquement ta variable d'env
api   = HfApi(token=token)               # tu passes maintenant le token ici

# 1) Créer le repo (pas besoin de repasser token)
api.create_repo(
    repo_id="alexandre-cameron-borges/ctr-model",
    private=True,
    exist_ok=True
)

# 2) Uploader le .pt
api.upload_file(
    path_or_fileobj="best_ctr_model.pt",
    path_in_repo="best_ctr_model.pt",
    repo_id="alexandre-cameron-borges/ctr-model"
)